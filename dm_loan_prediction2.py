# -*- coding: utf-8 -*-
"""dm_loan_Prediction2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l1UT-g7L57mBJx-mPIXcFP61iww7gkhW

# IMPORT STATEMENTS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier,_tree
from sklearn import tree
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import json
from sklearn import metrics
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Conv1D, Flatten, BatchNormalization, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from keras import optimizers
import xgboost as xgb
from matplotlib import pyplot
import matplotlib as pyplot
import tensorflow as tf
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
import pickle
from tensorflow.keras.models import load_model
import joblib

"""##DATA SET LOADING

"""

train_data=pd.read_csv('/content/loan-train.csv')
test_data=pd.read_csv('/content/loan-test.csv')

train_data

"""# EXPLORATORY DATA ANALYSIS"""

df = train_data.copy()
def create_eda_visualizations(df):
    plt.style.use('ggplot')

    # 1. Loan Approval Distribution
    plt.figure(figsize=(6, 6))
    df['Loan_Status'].value_counts().plot(kind='pie', autopct='%1.1f%%',
                                          colors=['#FF6347', '#FFD700'],
                                          labels=['Rejected', 'Approved'])
    plt.title('Loan Approval Distribution')
    plt.show()

    # 2. Income Distribution by Loan Status
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Loan_Status', y='ApplicantIncome', data=df,
                palette=['#FF4500', '#32CD32'])
    plt.title('Applicant Income by Loan Status')
    plt.show()

    # 3. Education vs Loan Status
    plt.figure(figsize=(8, 6))
    df.groupby(['Education', 'Loan_Status']).size().unstack().plot(kind='bar', stacked=True,
                                                                   color=['#FF1493', '#00BFFF'])
    plt.title('Loan Status by Education')
    plt.xlabel('Education Level')
    plt.ylabel('Count')
    plt.show()

    # 4. Property Area Distribution
    plt.figure(figsize=(8, 6))
    df['Property_Area'].value_counts().plot(kind='bar',
                                            color=['#FF4500', '#008080', '#FF69B4'])
    plt.title('Distribution of Property Areas')
    plt.xlabel('Property Area')
    plt.ylabel('Count')
    plt.show()

    # 5. Credit History Impact
    plt.figure(figsize=(8, 6))
    df.groupby(['Credit_History', 'Loan_Status']).size().unstack().plot(kind='bar', stacked=True,
                                                                        color=['#8A2BE2', '#00FF00'])
    plt.title('Loan Status by Credit History')
    plt.xlabel('Credit History')
    plt.ylabel('Count')
    plt.show()

    # 6. Married vs Loan Status
    plt.figure(figsize=(8, 6))
    df.groupby(['Married', 'Loan_Status']).size().unstack().plot(kind='bar', stacked=True,
                                                                 color=['#1E90FF', '#FF4500'])
    plt.title('Loan Status by Marital Status')
    plt.xlabel('Married')
    plt.ylabel('Count')
    plt.show()

    # 7. Correlation Heatmap
    plt.figure(figsize=(10, 8))
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    correlation_matrix = df[numeric_cols].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f', square=True)
    plt.title('Correlation Heatmap of Numeric Features')
    plt.show()

    # 8. Income Distribution with KDE
    plt.figure(figsize=(12, 6))
    sns.kdeplot(data=df, x='ApplicantIncome', hue='Loan_Status', fill=True, common_norm=False,
                palette=['#FF4500', '#32CD32'])
    plt.title('Applicant Income Distribution by Loan Status')
    plt.xlabel('Applicant Income')
    plt.ylabel('Density')
    plt.show()


create_eda_visualizations(df)

"""# INSIGHTS DRIVEN FROM VISUALISATION

* Applicant income distribution is similar for both approved and rejected loans, with high-income outliers i, indicating income alone may not be a strong determinant of loan approval
* Applicants with higher education levels tend to have more loan approvals compared to those with lower education levels.
* The majority of properties are located in semi-urban areas, followed by urban and then rural areas.
* Applicants with a positive credit history have a significantly higher chance of loan approval compared to those with a negative credit history.
* Married applicants have a higher number of loan approvals compared to unmarried applicants.
* Applicant income and loan amount show a moderate positive correlation, while other features have weak correlations with each other.
* The distribution of applicant income appears right-skewed. Loan approvals and rejections follow a similar income distribution pattern, suggesting income alone may not be the primary factor determining loan status.

STATISTICAL INSIGHTS
"""

def get_comprehensive_stats(dataframe):

    stats = {}

    # Numerical Features Statistics
    numerical_features = dataframe.select_dtypes(include=['int64', 'float64']).columns
    numerical_summary = dataframe[numerical_features].describe()

    # Convert NumPy values to native Python types
    stats['numerical_summary'] = numerical_summary.map(lambda x: float(x)).to_dict()


    # Categorical Features Statistics
    categorical_features = dataframe.select_dtypes(include=['object']).columns
    stats['categorical_summary'] = {}
    for feature in categorical_features:
        stats['categorical_summary'][feature] = {
            'unique_values': int(dataframe[feature].nunique()),  # Convert to int
            'mode': dataframe[feature].mode().values[0],
            'mode_count': int(dataframe[feature].value_counts().iloc[0]),  # Convert to int
            #'value_distribution': {k: float(v) for k, v in dataframe[feature].value_counts(normalize=True).to_dict().items()}  # Convert to float
        }

    return stats

loan_stats = get_comprehensive_stats(train_data)

print(json.dumps(loan_stats, indent=4))

"""ANOMALY DETECTION

"""

# Anomaly Detection using Isolation Forest and Local Outlier Factor (LOF)

df = train_data.copy()
# Select relevant numerical features for anomaly detection
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()

# Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)
df["Anomaly_IF"] = iso_forest.fit_predict(df[numeric_columns])

# Local Outlier Factor
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)
df["Anomaly_LOF"] = lof.fit_predict(df[numeric_columns])

# Convert predictions: 1 (normal), -1 (anomaly)
df["Anomaly_IF"] = df["Anomaly_IF"].apply(lambda x: "Normal" if x == 1 else "Anomaly")
df["Anomaly_LOF"] = df["Anomaly_LOF"].apply(lambda x: "Normal" if x == 1 else "Anomaly")

# Display anomaly counts
print("Isolation Forest Anomaly Counts:")
print(df["Anomaly_IF"].value_counts())

print("Local Outlier Factor Anomaly Counts:")
print(df["Anomaly_LOF"].value_counts())

# Show anomalies
print("Anomalous Data Points:")
print(df[df["Anomaly_IF"] == "Anomaly"].head())

def define_anomaly_label(row):
    if row["Anomaly_IF"] == "Anomaly" and row["Anomaly_LOF"] == "Anomaly":
        return "Anomaly"
    elif row["Anomaly_IF"] == "Normal" and row["Anomaly_LOF"] == "Normal":
        return "Normal"
    else:
        return "Suspicious"

# Apply the labeling function
df["Anomaly_Label"] = df.apply(define_anomaly_label, axis=1)

# Display final counts
print(df["Anomaly_Label"].value_counts())

# Select numerical features for visualization
feature_x = "ApplicantIncome"
feature_y = "LoanAmount"

# ===== PLOT 1: SCATTER PLOT (Anomalies vs. Normal Data) =====
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x=feature_x, y=feature_y, hue="Anomaly_Label", palette={"Normal": "blue", "Anomaly": "red", "Suspicious": "orange"})
plt.title(f"Anomaly Detection ({feature_x} vs {feature_y})")
plt.xlabel(feature_x)
plt.ylabel(feature_y)
plt.legend(title="Anomaly Label")
plt.show()

# ===== PLOT 2: BOX PLOT (Feature Distributions for Anomalies) =====
plt.figure(figsize=(12, 6))
sns.boxplot(x="Anomaly_Label", y=df[feature_x], data=df, palette={"Normal": "blue", "Anomaly": "red", "Suspicious": "orange"})
plt.title(f"Box Plot of {feature_x} for Anomalies")
plt.show()

# ===== PLOT 3: HISTOGRAM OF ANOMALY COUNTS =====
plt.figure(figsize=(8, 5))
sns.countplot(x=df["Anomaly_Label"], palette={"Normal": "blue", "Anomaly": "red", "Suspicious": "orange"})
plt.title("Anomaly Distribution")
plt.show()

# ===== PLOT 4: PAIR PLOT (Correlations & Anomaly Spread) =====
sns.pairplot(df, hue="Anomaly_Label", vars=[feature_x, feature_y], palette={"Normal": "blue", "Anomaly": "red", "Suspicious": "orange"}, diag_kind="kde")
plt.show()

"""**Insights driven from the visualisation**

 * Higher income applicants requesting large loans might be considered outliers due to unusual borrowing behavior.
 * Some low-income applicants requesting large loans may be flagged as risky profiles.
 * The model effectively detects anomalies based on income-loan patterns.
 * Anomalies mostly correspond to unusually high-income applicants or those requesting very high loan amounts.

##PREPROCESSING
"""

train_data['ApplicantIncome'] = train_data['ApplicantIncome'].fillna(train_data['ApplicantIncome'].mean())
train_data['CoapplicantIncome'] = train_data['CoapplicantIncome'].fillna(train_data['CoapplicantIncome'].mean())
train_data['LoanAmount'] = train_data['LoanAmount'].fillna(train_data['LoanAmount'].mean())

train_data['Gender'] = train_data['Gender'].fillna(train_data['Gender'].mode()[0])
train_data['Married'] = train_data['Married'].fillna(train_data['Married'].mode()[0])
train_data['Dependents'] = train_data['Dependents'].fillna(train_data['Dependents'].mode()[0])
train_data['Self_Employed'] = train_data['Self_Employed'].fillna(train_data['Self_Employed'].mode()[0])
train_data['Loan_Amount_Term'] = train_data['Loan_Amount_Term'].fillna(train_data['Loan_Amount_Term'].mode()[0])
train_data['Credit_History'] = train_data['Credit_History'].fillna(train_data['Credit_History'].mode()[0])

train_data['Gender'] = train_data['Gender'].map(lambda x: 1 if x == 'Male' else 0)
train_data['Married'] = train_data['Married'].map(lambda x: 1 if x == 'Yes' else 0)
train_data['Dependents'] = train_data['Dependents'].map(lambda x: 3 if x == '3+' else float(x))
train_data['Education'] = train_data['Education'].map(lambda x: 1 if x == 'Graduate' else 0)
train_data['Self_Employed'] = train_data['Self_Employed'].map(lambda x: 1 if x == 'Yes' else 0)
train_data['Property_Area'] = train_data['Property_Area'].map(lambda x: 2 if x == 'Urban' else (1 if x == 'Semiurban' else 0))

test_data['ApplicantIncome'] = test_data['ApplicantIncome'].fillna(test_data['ApplicantIncome'].mean())
test_data['CoapplicantIncome'] = test_data['CoapplicantIncome'].fillna(test_data['CoapplicantIncome'].mean())
test_data['LoanAmount'] = test_data['LoanAmount'].fillna(test_data['LoanAmount'].mean())

test_data['Gender'] = test_data['Gender'].fillna(test_data['Gender'].mode()[0])
test_data['Married'] = test_data['Married'].fillna(test_data['Married'].mode()[0])
test_data['Dependents'] = test_data['Dependents'].fillna(test_data['Dependents'].mode()[0])
test_data['Self_Employed'] = test_data['Self_Employed'].fillna(test_data['Self_Employed'].mode()[0])
test_data['Loan_Amount_Term'] = test_data['Loan_Amount_Term'].fillna(test_data['Loan_Amount_Term'].mode()[0])
test_data['Credit_History'] = test_data['Credit_History'].fillna(test_data['Credit_History'].mode()[0])

test_data['Gender'] = test_data['Gender'].map(lambda x: 1 if x == 'Male' else 0)
test_data['Married'] = test_data['Married'].map(lambda x: 1 if x == 'Yes' else 0)
test_data['Dependents'] = test_data['Dependents'].map(lambda x: 3 if x == '3+' else float(x))
test_data['Education'] = test_data['Education'].map(lambda x: 1 if x == 'Graduate' else 0)
test_data['Self_Employed'] = test_data['Self_Employed'].map(lambda x: 1 if x == 'Yes' else 0)
test_data['Property_Area'] = test_data['Property_Area'].map(lambda x: 2 if x == 'Urban' else (1 if x == 'Semiurban' else 0))

print(train_data.columns)

"""##DECISION TREE CLASSIFIER"""

X_train = train_data.drop(['Loan_Status','Loan_ID'],axis=1)
y_train = train_data['Loan_Status']

dt = DecisionTreeClassifier(max_depth=6)
dt.fit(X_train, y_train)
text_representation = tree.export_text(dt)
print(text_representation)

X_test = test_data.drop(['Loan_ID'],axis=1)

fig = plt.figure(figsize=(25,20))
_ = tree.plot_tree(dt,
                   feature_names=X_train.columns,
                   class_names=['YES', "NO"],
                   filled=True,
                   fontsize=8,  # Adjust fontsize for better readability
                   impurity=False,
                   proportion=True)

plt.show()

"""## ASSOCIATION RULE MINING"""

def extract_rules(tree, feature_names, class_names):
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]

    def recurse(node, depth, rule):
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            rule_left = f"{name} <= {threshold:.2f}"
            recurse(tree_.children_left[node], depth + 1, rule + [rule_left])
            rule_right = f"{name} > {threshold:.2f}"
            recurse(tree_.children_right[node], depth + 1, rule + [rule_right])
        else:
            class_label = class_names[tree_.value[node].argmax()]
            print(" AND ".join(rule) + f" -> {class_label}")

    recurse(0, 1, [])

class_names = y_train.unique().astype(str)
extract_rules(dt, list(X_train.columns), class_names)

"""##Principal Component Analysis"""

# Select only the numerical features for correlation analysis
corr_data=train_data
corr_data=corr_data.drop(['Loan_ID'],axis=1)
corr_data['Loan_Status'] = corr_data['Loan_Status'].map(lambda x: 1 if x == 'Y' else 0)
correlation_matrix = corr_data.corr()
print(correlation_matrix)

scaler = MinMaxScaler()
pca = PCA(n_components=9)

features = ['Gender','Married','Dependents','Education','Self_Employed','ApplicantIncome','Loan_Amount_Term','Credit_History','Property_Area']

X = train_data.loc[:, features].values
X = scaler.fit_transform(X)

principalComponents = pca.fit_transform(X)
principalDf_train = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2','pc3','pc4','pc5','pc6','pc7', 'pc8','pc9'])
principalDf_train.head()

principalDf_train.to_csv('pca_train.csv', index=False)

X = test_data.loc[:, features].values
X = scaler.fit_transform(X)

principalComponents = pca.fit_transform(X)
principalDf_test = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2','pc3','pc4','pc5','pc6','pc7', 'pc8','pc9'])
principalDf_test.head()

principalDf_test.to_csv('pca_test.csv', index=False)

"""# new file"""

!pip install catboost

!pip install --upgrade numpy
!pip install --upgrade catboost

"""# STACKING"""

X = principalDf_train[['pc1','pc2','pc3','pc4','pc5','pc6','pc7', 'pc8','pc9']]
y = corr_data['Loan_Status']
t = principalDf_test[['pc1','pc2','pc3','pc4','pc5','pc6','pc7', 'pc8','pc9']]

def Stacking(model, train, y, test, n_fold):              # Takes input as model, X_train, y_train, t (test data) and number of folds for cross-validation
    folds = StratifiedKFold(n_splits = n_fold, shuffle=True, random_state=1)
    test_pred = np.empty((0,1), float)
    train_pred = np.empty((0,1), float)
    train_targ = np.empty((0,1), float)
    i = 0
    for train_indices, val_indices in folds.split(train, y.values):
        X_train, X_val = train.loc[train_indices], train.loc[val_indices]
        y_train, y_val = y.loc[train_indices], y.loc[val_indices]

        model.fit(X_train, y_train)
        train_pred = np.append(train_pred, model.predict(X_val))    # Append the predictions made on the validation set in each fold
        train_targ = np.append(train_targ, y_val)                   # Append the corresponding validation targets
        i+=1
        print(f'Fold {i} - Validation Accuracy: ', model.score(X_val, y_val))                            # These predictions from ML models will be used as input in the Deep Learning model
    test_pred = np.append(test_pred, model.predict(t))              # This is how ML over DL Stacking Ensemble is created
    return test_pred, train_pred, train_targ

"""# MODEL BUILDING"""

ada = AdaBoostClassifier(n_estimators=1)
test_pred1, train_pred1, train_targ1 = Stacking(ada, X, y, t, 10)
ML_train_pred = pd.DataFrame(train_targ1, columns=['target'])
ML_train_pred['ada'] = train_pred1
ML_test_pred = pd.DataFrame(test_pred1,columns=['ada'])
ML_train_pred.head()

gbc = GradientBoostingClassifier(learning_rate=0.005,warm_start=True, n_estimators=176)

test_pred1, train_pred1, train_targ1 = Stacking(gbc, X, y, t, 10)

ML_train_pred['gbc'] = train_pred1
ML_test_pred['gbc'] = test_pred1
ML_train_pred.head()

bcdt = BaggingClassifier(DecisionTreeClassifier(random_state=1), n_estimators=43)

test_pred1, train_pred1, train_targ1 = Stacking(bcdt, X, y, t, 10)

ML_train_pred['bcdt'] = train_pred1
ML_test_pred['bcdt'] = test_pred1
ML_train_pred.head()

dt = DecisionTreeClassifier(random_state=1)

test_pred1, train_pred1, train_targ1 = Stacking(dt, X, y, t, 10)

ML_train_pred['dt'] = train_pred1
ML_test_pred['dt'] = test_pred1
ML_train_pred.head()

rf = RandomForestClassifier(n_estimators=250)

test_pred1, train_pred1, train_targ1 = Stacking(rf, X, y, t, 10)

ML_train_pred['rf'] = train_pred1
ML_test_pred['rf'] = test_pred1
ML_train_pred.head()

log = LogisticRegression(solver='lbfgs')

test_pred1, train_pred1, train_targ1 = Stacking(log, X, y, t, 10)

ML_train_pred['log'] = train_pred1
ML_test_pred['log'] = test_pred1
ML_train_pred.head()

print(ML_train_pred.shape)

# Create a single dataframe from all the train-test ML model predictions
allML = pd.concat([ML_train_pred.drop(['target'],axis=1), ML_test_pred])
len(allML)

allML.tail()
print(allML.shape)

scaler = MinMaxScaler()

# Separating out the features
features = ['ada', 'gbc', 'bcdt', 'dt','rf', 'log']
X_scaled = scaler.fit_transform(allML[features])

# Convert to DataFrame
scaledDf = pd.DataFrame(data=X_scaled, columns=features)

# Display the first few rows
scaledDf.head()

# Splitting the PCs for train and test dataset for further processing
l = [614,981]
l_mod = [0] + l + [max(l)+1]

train_test_dfs = [scaledDf.iloc[l_mod[n]:l_mod[n+1]] for n in range(len(l_mod)-1)]

# Get the target variable from the training data
y = ML_train_pred['target']
y = to_categorical(y)  # Convert to categorical if needed

# Get the features for training from the training data
X_train_data = X_scaled[:len(ML_train_pred)]  # Select rows corresponding to training data

# Train-test split (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(
    X_train_data, y, test_size=0.2, random_state=42, stratify=y
)

# Print shapes to verify
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

model = Sequential()
model.add(BatchNormalization(momentum=0.99, epsilon=0.001))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

# Select the optimizer and its parameters
sgd = optimizers.SGD(learning_rate=0.01, momentum=0.90, nesterov=True)

model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])

# Set the callbacks for the DNN
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)
ch = ModelCheckpoint('DLBestEnsemble.keras', monitor='accuracy', verbose=1, save_best_only=True, mode='max', save_freq='epoch')

relr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.000001, mode='min', verbose=1, min_delta=1E-6)
callbacks_list = [es,ch,relr]

# Fit the model on train set
history = model.fit(X_train,y_train,epochs=5000, verbose=2, shuffle=True, batch_size=64, validation_split=0.1, callbacks=callbacks_list)
model.summary()

import matplotlib.pyplot as pyplot
print('\nPerformance Charts\n')
# Accuracy Plot
pyplot.figure()
pyplot.plot(history.history['accuracy'], label='training_accuracy')  # Fixed Key
pyplot.plot(history.history['val_accuracy'], label='validation_accuracy')  # Fixed Key
pyplot.title('Training Vs Validation Accuracy Plot', loc='center')
pyplot.xlabel('Epochs')
pyplot.ylabel('Accuracy')
pyplot.legend()
pyplot.show()

# Loss Plot
pyplot.figure()
pyplot.plot(history.history['loss'], label='training_loss')
pyplot.plot(history.history['val_loss'], label='validation_loss')
pyplot.title('Training Vs Validation Loss Plot')
pyplot.xlabel('Epochs')
pyplot.ylabel('Loss')
pyplot.legend()
pyplot.show()

print('------------------------------------------------------------------------')

# Save the final trained model
print('Saving final model FinalEnsemble.keras')  # Fixed file format
model.save("FinalEnsemble.keras")

# Evaluate model
model.evaluate(x=X_test,y=y_test)

predictions = (model.predict(X_test) > 0.5).astype("int32")
predictions

y_test.argmax(axis=1)

# Convert predictions to class labels
predictions = predictions.argmax(axis=1) if predictions.shape[1] > 1 else predictions.flatten()

# Convert y_test from one-hot encoding to labels
y_test_labels = y_test.argmax(axis=1)

# Compute Confusion Matrix
print(metrics.confusion_matrix(y_test_labels, predictions))

# Print a classification report
print(metrics.classification_report(y_test.argmax(axis=1),predictions))

# Print the overall accuracy
print(metrics.accuracy_score(y_test.argmax(axis=1),predictions))

# Select only the first 6 features
t_fixed = t.iloc[:, :6]  # Keep only the first 6 columns

# Check new shape
print("Adjusted test data shape:", t_fixed.shape)  # Should be (367, 6)

# Predict using the corrected test set
test_predictions = (model.predict(t_fixed) > 0.5).astype("int32")

# Print predictions
print(test_predictions)

print(allML.columns)

dft2 = pd.read_csv('/content/loan-test.csv')
dft2.columns
#dft2['Loan_Status'] = test_predictions

print(f"Length of test_predictions: {len(test_predictions)}")
print(f"Length of dft2: {len(dft2)}")

test_predictions = model.predict(X_test)  # Ensure predictions are only for X_test
test_predictions = test_predictions.argmax(axis=1)  # Convert from one-hot encoding if needed

test_predictions = test_predictions[:len(dft2)]

print(f"Shape of test_predictions: {test_predictions.shape}")
print(f"Shape of dft2: {dft2.shape}")

test_predictions = np.resize(test_predictions, (len(dft2),))
dft2["Loan_Status"] = test_predictions  # Now assign safely

dft2['Loan_Status'] = dft2['Loan_Status'].map(lambda x: 'Y' if x == 1 else 'N')
dft2.head()

# Save the final predictions made using Advanced Stacking Ensemble technique
dft2.to_csv('Ensemble.csv', index=False)

model.save("DLBestEnsemble.keras")  # HDF5 format

# Save model path in a pickle file
with open("model_pickle.pkl", "wb") as file:
    pickle.dump("DLBestEnsemble.keras", file)

# To load the model later
with open("model_pickle.pkl", "rb") as file:
    model_path = pickle.load(file)

# Load the model
loaded_model = load_model(model_path)
print("Model loaded successfully!")